{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:31:44.794537Z",
     "iopub.status.busy": "2025-12-22T08:31:44.794258Z",
     "iopub.status.idle": "2025-12-22T08:31:57.365172Z",
     "shell.execute_reply": "2025-12-22T08:31:57.364541Z",
     "shell.execute_reply.started": "2025-12-22T08:31:44.794505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import functools\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "import torch as t\n",
    "from threading import Thread\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-22T08:32:02.132685Z",
     "iopub.status.busy": "2025-12-22T08:32:02.132384Z",
     "iopub.status.idle": "2025-12-22T08:32:02.827404Z",
     "shell.execute_reply": "2025-12-22T08:32:02.826800Z",
     "shell.execute_reply.started": "2025-12-22T08:32:02.132656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-1.5b/2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:32:03.009658Z",
     "iopub.status.busy": "2025-12-22T08:32:03.009360Z",
     "iopub.status.idle": "2025-12-22T08:32:52.413389Z",
     "shell.execute_reply": "2025-12-22T08:32:52.412808Z",
     "shell.execute_reply.started": "2025-12-22T08:32:03.009632Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 08:32:05.715045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766392325.936810      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766392326.004621      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766392326.553155      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766392326.553184      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766392326.553187      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766392326.553189      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype = t.bfloat16,\n",
    "    device_map = \"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:32:52.415350Z",
     "iopub.status.busy": "2025-12-22T08:32:52.414701Z",
     "iopub.status.idle": "2025-12-22T08:32:52.419320Z",
     "shell.execute_reply": "2025-12-22T08:32:52.418592Z",
     "shell.execute_reply.started": "2025-12-22T08:32:52.415323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:32:52.440034Z",
     "iopub.status.busy": "2025-12-22T08:32:52.439788Z",
     "iopub.status.idle": "2025-12-22T08:32:52.444264Z",
     "shell.execute_reply": "2025-12-22T08:32:52.443534Z",
     "shell.execute_reply.started": "2025-12-22T08:32:52.440012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# receiver_heads = [(15,  7),(0,  8),(0,  0), (0,  6), (21,  5), (1,  3), (14,  8), (21,  2), (15, 11), (12,  8), (7, 2), (9, 2), (6,  6), (11, 11), (6,  2), (21,  3), (14,  7), (8,  4), (11,  1), (4,  9)]\n",
    "receiver_heads  = [(21, 5), (21, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:33:49.076452Z",
     "iopub.status.busy": "2025-12-22T08:33:49.075678Z",
     "iopub.status.idle": "2025-12-22T08:33:49.081774Z",
     "shell.execute_reply": "2025-12-22T08:33:49.080636Z",
     "shell.execute_reply.started": "2025-12-22T08:33:49.076421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "d_head = model.config.hidden_size // model.config.num_attention_heads\n",
    "def scale_receiver_heads_hook(module, input, head_idx, scale=2):\n",
    "    full_attn_matrix = input[0].clone()  #removes the batch dim\n",
    "    b, s, h = full_attn_matrix.shape\n",
    "    reshaped_attn_matrix = full_attn_matrix.view(b, s, -1, d_head)\n",
    "    reshaped_attn_matrix[:, :, head_idx, :] *= scale\n",
    "    return (reshaped_attn_matrix.view(b, s, -1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:33:28.736202Z",
     "iopub.status.busy": "2025-12-22T08:33:28.735615Z",
     "iopub.status.idle": "2025-12-22T08:33:28.753057Z",
     "shell.execute_reply": "2025-12-22T08:33:28.752293Z",
     "shell.execute_reply.started": "2025-12-22T08:33:28.736171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#get the problems\n",
    "\n",
    "records = []\n",
    "with open(\"/kaggle/input/evalsdataset/evals_final.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        records.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:33:32.464319Z",
     "iopub.status.busy": "2025-12-22T08:33:32.464018Z",
     "iopub.status.idle": "2025-12-22T08:33:32.471600Z",
     "shell.execute_reply": "2025-12-22T08:33:32.470708Z",
     "shell.execute_reply.started": "2025-12-22T08:33:32.464292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#get the answers\n",
    "\n",
    "with open('/kaggle/input/answers/answers.pkl', 'rb') as f:\n",
    "    actual_answers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:33:54.031509Z",
     "iopub.status.busy": "2025-12-22T08:33:54.031129Z",
     "iopub.status.idle": "2025-12-22T08:33:54.038030Z",
     "shell.execute_reply": "2025-12-22T08:33:54.037294Z",
     "shell.execute_reply.started": "2025-12-22T08:33:54.031481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_streaming_output(ids, max_new_tokens=500):\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,\n",
    "        skip_prompt=True\n",
    "    )\n",
    "\n",
    "    g_kwargs = dict(\n",
    "        **ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    thread = Thread(target=model.generate, kwargs=g_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    buf = StringIO()\n",
    "    for chunk in streamer:\n",
    "        buf.write(chunk)\n",
    "\n",
    "    thread.join()\n",
    "    return buf.getvalue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:35:27.964277Z",
     "iopub.status.busy": "2025-12-22T08:35:27.963969Z",
     "iopub.status.idle": "2025-12-22T08:35:28.107802Z",
     "shell.execute_reply": "2025-12-22T08:35:28.106806Z",
     "shell.execute_reply.started": "2025-12-22T08:35:27.964244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm /kaggle/working/evals_scale_10.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:35:34.623315Z",
     "iopub.status.busy": "2025-12-22T08:35:34.622983Z",
     "iopub.status.idle": "2025-12-22T08:35:34.632900Z",
     "shell.execute_reply": "2025-12-22T08:35:34.632169Z",
     "shell.execute_reply.started": "2025-12-22T08:35:34.623284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def boost_heads_generate(num_questions: int, scale: int = 5):\n",
    "    out_f = open(f\"evals_scale_{scale}.jsonl\", \"a\", encoding=\"utf-8\")\n",
    "    \n",
    "    for i in tqdm(range(num_questions)):\n",
    "        problem = records[i][\"problem\"]\n",
    "            \n",
    "        boosting_hooks = []\n",
    "        try:\n",
    "            for layer, head in receiver_heads:\n",
    "                target_module = model.model.layers[layer].self_attn.o_proj\n",
    "                target_module.output_attentions = True\n",
    "            \n",
    "                hook = target_module.register_forward_pre_hook(\n",
    "                    functools.partial(scale_receiver_heads_hook, head_idx = head, scale = scale)\n",
    "                )\n",
    "                boosting_hooks.append(hook)\n",
    "        \n",
    "            \n",
    "            problem = records[i][\"problem\"]\n",
    "        \n",
    "            full_prompt = (\n",
    "                \"<|User|>\"\n",
    "                + problem\n",
    "                + \"<|Assistant|><think>\"\n",
    "            )\n",
    "            input_ids = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "        \n",
    "            answer_text = generate_streaming_output(input_ids, max_new_tokens=4096)\n",
    "        \n",
    "            record = {\n",
    "                \"q_id\": i,\n",
    "                \"problem\": problem,\n",
    "                \"cot\": answer_text,\n",
    "                \"actual_ans\": actual_answers[i]\n",
    "            }\n",
    "        \n",
    "            out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "            out_f.flush() \n",
    "\n",
    "            del input_ids\n",
    "            del answer_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error occured: {e}\")\n",
    "\n",
    "        finally:\n",
    "            for hook in boosting_hooks:\n",
    "                hook.remove()  \n",
    "\n",
    "    out_f.close()                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T08:35:36.795929Z",
     "iopub.status.busy": "2025-12-22T08:35:36.795005Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe95be178434153a5d258e730ad3e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boost_heads_generate(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9094285,
     "sourceId": 14253258,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9094339,
     "sourceId": 14253343,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 225262,
     "modelInstanceId": 204042,
     "sourceId": 256574,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
